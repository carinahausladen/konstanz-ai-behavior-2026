{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEAT Tutorial: Measuring Gender Bias Step-by-Step\n",
    "\n",
    "**Goal:** Understand if a language model associates male/female words differently with career/family words.\n",
    "\n",
    "**The Big Idea:**  \n",
    "If a model is unbiased, \"man\" and \"woman\" should be equally close to \"career\" words.  \n",
    "If there's bias, one gender will be systematically closer to certain concepts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import\n",
    "\n",
    "We only need a few simple tools."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:36:03.144365Z",
     "start_time": "2026-01-07T16:36:03.010660Z"
    }
   },
   "source": [
    "# Install if needed\n",
    "!pip install transformers torch numpy"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:36:06.042247Z",
     "start_time": "2026-01-07T16:36:03.820977Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Use CPU or GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using: {device}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carinah/PycharmProjects/Konstanz_course_2026/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load a Simple Model\n",
    "\n",
    "We'll use BERT - it's a popular language model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:36:11.135646Z",
     "start_time": "2026-01-07T16:36:09.968317Z"
    }
   },
   "source": [
    "# Load BERT\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "model.eval()  # Put in evaluation mode\n",
    "\n",
    "print(\"Model loaded!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Our Test Words\n",
    "\n",
    "**Super simple example:**\n",
    "- **Target Set 1 (Male):** man, he\n",
    "- **Target Set 2 (Female):** woman, she  \n",
    "- **Attribute Set 1 (Career):** work, salary\n",
    "- **Attribute Set 2 (Family):** home, family"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:36:12.434434Z",
     "start_time": "2026-01-07T16:36:12.410711Z"
    }
   },
   "source": [
    "male_words = ['man', 'he']\n",
    "female_words = ['woman', 'she']\n",
    "career_words = ['work', 'salary']\n",
    "family_words = ['home', 'family']\n",
    "print(\"Test words defined!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test words defined!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Get Word Embeddings\n",
    "\n",
    "An **embedding** is just a list of numbers that represents a word.\n",
    "\n",
    "Let's get the embedding for one word first to understand:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:36:14.691192Z",
     "start_time": "2026-01-07T16:36:14.055902Z"
    }
   },
   "source": [
    "# Example: Get embedding for \"man\"\n",
    "word = \"man\"\n",
    "\n",
    "# Step 1: Convert word to numbers that BERT understands\n",
    "inputs = tokenizer(word, return_tensors='pt').to(device)\n",
    "print(f\"Input IDs: {inputs['input_ids']}\")\n",
    "\n",
    "# Step 2: Pass through BERT\n",
    "with torch.no_grad():  # Don't calculate gradients (we're not training)\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Step 3: Get the embedding (it's hidden in the output)\n",
    "# Shape: [1, sequence_length, 768]\n",
    "# We take the middle token (index 1) and ignore [CLS] and [SEP]\n",
    "embedding = outputs.last_hidden_state[0, 1, :]\n",
    "\n",
    "print(f\"\\nEmbedding shape: {embedding.shape}\")\n",
    "print(f\"First 10 numbers: {embedding[:10].cpu().numpy()}\")\n",
    "print(f\"\\nThis is a vector of {len(embedding)} numbers representing 'man'\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 101, 2158,  102]])\n",
      "\n",
      "Embedding shape: torch.Size([768])\n",
      "First 10 numbers: [-0.30896628 -0.10018251 -0.25786588 -0.7900988   0.21146199  0.36500245\n",
      "  0.7095498  -0.2389967  -0.23208368 -1.283527  ]\n",
      "\n",
      "This is a vector of 768 numbers representing 'man'\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a simple function to get embeddings for any word:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:36:17.408056Z",
     "start_time": "2026-01-07T16:36:17.278677Z"
    }
   },
   "source": [
    "def get_embedding(word):\n",
    "    \"\"\"Get the embedding vector for a word.\"\"\"\n",
    "    inputs = tokenizer(word, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Get middle token, convert to numpy\n",
    "    embedding = outputs.last_hidden_state[0, 1, :].cpu().numpy()\n",
    "    return embedding\n",
    "\n",
    "# Test it\n",
    "man_emb = get_embedding('man')\n",
    "woman_emb = get_embedding('woman')\n",
    "\n",
    "print(f\"man embedding: {man_emb[:5]}...\")\n",
    "print(f\"woman embedding: {woman_emb[:5]}...\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man embedding: [-0.30896628 -0.10018251 -0.25786588 -0.7900988   0.21146199]...\n",
      "woman embedding: [-0.6725328  -0.47752842 -0.02868308 -0.71078324  0.1911808 ]...\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Measure Similarity Between Words\n",
    "\n",
    "**Cosine similarity** tells us how similar two embeddings are:  \n",
    "- 1.0 = identical  \n",
    "- 0.0 = unrelated  \n",
    "- -1.0 = opposite\n",
    "\n",
    "Formula: similarity = (A · B) / (|A| × |B|)\n",
    "\n",
    "### Worked Example (Do This by Hand ✍️)\n",
    "Grab a pen and paper and work through each step yourself. This is basic vector math, and you should be able to calculate it by hand.\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "B =\n",
    "\\begin{bmatrix}\n",
    "2 \\\\\n",
    "4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Dot product**\n",
    "\n",
    "$$\n",
    "A \\cdot B\n",
    "=\n",
    "(1 \\times 2) + (2 \\times 4)\n",
    "=\n",
    "10\n",
    "$$\n",
    "\n",
    "**Magnitudes**\n",
    "\n",
    "$$\n",
    "\\lVert A \\rVert\n",
    "=\n",
    "\\sqrt{1^2 + 2^2}\n",
    "=\n",
    "\\sqrt{5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lVert B \\rVert\n",
    "=\n",
    "\\sqrt{2^2 + 4^2}\n",
    "=\n",
    "\\sqrt{20}\n",
    "$$\n",
    "\n",
    "**Cosine similarity**\n",
    "\n",
    "$$\n",
    "\\text{CosineSimilarity}(A, B)\n",
    "=\n",
    "\\frac{10}{\\sqrt{5} \\cdot \\sqrt{20}}\n",
    "=\n",
    "1.0\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:36:41.689066Z",
     "start_time": "2026-01-07T16:36:41.552252Z"
    }
   },
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate how similar two vectors are.\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    magnitude1 = np.linalg.norm(vec1)\n",
    "    magnitude2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "# Test: How similar are \"man\" and \"woman\"?\n",
    "man_emb = get_embedding('man')\n",
    "woman_emb = get_embedding('woman')\n",
    "work_emb = get_embedding('work')\n",
    "\n",
    "\n",
    "sim_man_work = cosine_similarity(man_emb, work_emb)\n",
    "sim_woman_work = cosine_similarity(woman_emb, work_emb)\n",
    "\n",
    "\n",
    "print(f\"Similarity between 'man' and 'work': {sim_man_work:.2f}\")\n",
    "print(f\"Similarity between 'woman' and 'work': {sim_woman_work:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'man' and 'work': 0.70\n",
      "Similarity between 'woman' and 'work': 0.65\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Calculate Association for ONE Word\n",
    "\n",
    "**Association** = How much closer is a word to career vs. family?\n",
    "\n",
    "For example, for \"man\":  \n",
    "1. Calculate average similarity to career words (work, salary)  \n",
    "2. Calculate average similarity to family words (home, family)  \n",
    "3. Association = career_similarity - family_similarity\n",
    "\n",
    "**Positive association** = closer to career  \n",
    "**Negative association** = closer to family"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:37:17.108464Z",
     "start_time": "2026-01-07T16:37:16.882940Z"
    }
   },
   "source": [
    "# Let's calculate association for \"man\"\n",
    "target_word = 'man'\n",
    "\n",
    "# Get embedding for our target word\n",
    "target_emb = get_embedding(target_word)\n",
    "print(f\"Analyzing: {target_word}\\n\")\n",
    "\n",
    "# Step 1: Get embeddings for career words\n",
    "career_embeddings = [get_embedding(w) for w in career_words]\n",
    "print(f\"Career words: {career_words}\")\n",
    "\n",
    "# Step 2: Calculate similarity to each career word\n",
    "career_sims = [cosine_similarity(target_emb, career_emb) for career_emb in career_embeddings]\n",
    "print(f\"Similarities to career words: {[f'{s:.2f}' for s in career_sims]}\")\n",
    "\n",
    "# Step 3: Average similarity to career\n",
    "avg_career_sim = np.mean(career_sims)\n",
    "print(f\"Average similarity to career: {avg_career_sim:.2f}\\n\")\n",
    "\n",
    "# Step 4: Same for family words\n",
    "family_embeddings = [get_embedding(w) for w in family_words]\n",
    "print(f\"Family words: {family_words}\")\n",
    "family_sims = [cosine_similarity(target_emb, family_emb) for family_emb in family_embeddings]\n",
    "print(f\"Similarities to family words: {[f'{s:.2f}' for s in family_sims]}\")\n",
    "avg_family_sim = np.mean(family_sims)\n",
    "print(f\"Average similarity to family: {avg_family_sim:.2f}\\n\")\n",
    "\n",
    "# Step 5: Calculate association\n",
    "association = avg_career_sim - avg_family_sim\n",
    "print(f\"=\"*50)\n",
    "print(f\"Association for '{target_word}': {association:.2f}\")\n",
    "if association > 0:\n",
    "    print(f\"→ '{target_word}' is closer to CAREER words\")\n",
    "else:\n",
    "    print(f\"→ '{target_word}' is closer to FAMILY words\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing: man\n",
      "\n",
      "Career words: ['work', 'salary']\n",
      "Similarities to career words: ['0.70', '0.57']\n",
      "Average similarity to career: 0.64\n",
      "\n",
      "Family words: ['home', 'family']\n",
      "Similarities to family words: ['0.44', '0.70']\n",
      "Average similarity to family: 0.57\n",
      "\n",
      "==================================================\n",
      "Association for 'man': 0.06\n",
      "→ 'man' is closer to CAREER words\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a function to do this for any word:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:37:33.509099Z",
     "start_time": "2026-01-07T16:37:32.774262Z"
    }
   },
   "source": [
    "def calculate_association(target_word, attribute_set1, attribute_set2):\n",
    "    \"\"\"Calculate how associated a word is with set1 vs set2.\"\"\"\n",
    "    \n",
    "    # Get target embedding\n",
    "    target_emb = get_embedding(target_word)\n",
    "    \n",
    "    # Get embeddings for both attribute sets\n",
    "    set1_embeddings = [get_embedding(w) for w in attribute_set1]\n",
    "    set2_embeddings = [get_embedding(w) for w in attribute_set2]\n",
    "    \n",
    "    # Calculate average similarity to each set\n",
    "    avg_sim_set1 = np.mean([cosine_similarity(target_emb, emb) for emb in set1_embeddings])\n",
    "    avg_sim_set2 = np.mean([cosine_similarity(target_emb, emb) for emb in set2_embeddings])\n",
    "    \n",
    "    # Association = difference\n",
    "    association = avg_sim_set1 - avg_sim_set2\n",
    "    \n",
    "    return association\n",
    "\n",
    "# Test for all our gender words\n",
    "print(\"Associations (Career - Family):\\n\")\n",
    "for word in male_words + female_words:\n",
    "    assoc = calculate_association(word, career_words, family_words)\n",
    "    print(f\"{word:10s}: {assoc:+.3f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associations (Career - Family):\n",
      "\n",
      "man       : +0.064\n",
      "he        : +0.013\n",
      "woman     : +0.054\n",
      "she       : -0.004\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: The Full WEAT Score\n",
    "\n",
    "Now we calculate WEAT for **groups** of words:\n",
    "\n",
    "**WEAT Question:** Do male words associate more with career than female words do?\n",
    "\n",
    "**Steps:**\n",
    "1. Calculate association for each male word\n",
    "2. Calculate association for each female word  \n",
    "3. Compare the averages\n",
    "4. Standardize by dividing by standard deviation (this gives us effect size)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:38:02.702032Z",
     "start_time": "2026-01-07T16:38:02.003591Z"
    }
   },
   "source": [
    "# Step 1: Get associations for all male words\n",
    "print(\"Male word associations (Career - Family):\")\n",
    "male_associations = []\n",
    "for word in male_words:\n",
    "    assoc = calculate_association(word, career_words, family_words)\n",
    "    male_associations.append(assoc)\n",
    "    print(f\"  {word}: {assoc:+.3f}\")\n",
    "\n",
    "print(f\"\\nAverage for male words: {np.mean(male_associations):.3f}\\n\")\n",
    "\n",
    "# Step 2: Get associations for all female words\n",
    "print(\"Female word associations (Career - Family):\")\n",
    "female_associations = []\n",
    "for word in female_words:\n",
    "    assoc = calculate_association(word, career_words, family_words)\n",
    "    female_associations.append(assoc)\n",
    "    print(f\"  {word}: {assoc:+.3f}\")\n",
    "\n",
    "print(f\"\\nAverage for female words: {np.mean(female_associations):.3f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male word associations (Career - Family):\n",
      "  man: +0.064\n",
      "  he: +0.013\n",
      "\n",
      "Average for male words: 0.038\n",
      "\n",
      "Female word associations (Career - Family):\n",
      "  woman: +0.054\n",
      "  she: -0.004\n",
      "\n",
      "Average for female words: 0.025\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:38:11.049365Z",
     "start_time": "2026-01-07T16:38:11.024342Z"
    }
   },
   "source": [
    "# Step 3: Calculate the difference\n",
    "mean_male = np.mean(male_associations)\n",
    "mean_female = np.mean(female_associations)\n",
    "difference = mean_male - mean_female\n",
    "\n",
    "print(f\"\\nMean association for male words: {mean_male:+.3f}\")\n",
    "print(f\"Mean association for female words: {mean_female:+.3f}\")\n",
    "print(f\"Difference (male - female): {difference:+.3f}\")\n",
    "\n",
    "# Step 4: Calculate effect size (standardized difference)\n",
    "all_associations = male_associations + female_associations\n",
    "std_dev = np.std(all_associations, ddof=1)  # ddof=1 for sample std\n",
    "\n",
    "effect_size = difference / std_dev\n",
    "\n",
    "print(f\"\\nStandard deviation: {std_dev:.3f}\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"WEAT EFFECT SIZE: {effect_size:.3f}\")\n",
    "print(f\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean association for male words: +0.038\n",
      "Mean association for female words: +0.025\n",
      "Difference (male - female): +0.013\n",
      "\n",
      "Standard deviation: 0.033\n",
      "============================================================\n",
      "WEAT EFFECT SIZE: 0.405\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Interpret the Result\n",
    "\n",
    "**What does the effect size mean?**\n",
    "\n",
    "- **Positive effect size:** Male words are MORE associated with career than female words\n",
    "- **Negative effect size:** Female words are MORE associated with career than male words  \n",
    "- **Close to 0:** No difference (no bias)\n",
    "\n",
    "**Rule of thumb** (Cohen's d):\n",
    "- 0.2 = small effect\n",
    "- 0.5 = medium effect  \n",
    "- 0.8 = large effect\n",
    "- >1.0 = very large effect"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:38:39.524438Z",
     "start_time": "2026-01-07T16:38:39.498653Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if abs(effect_size) < 0.2:\n",
    "    magnitude = \"VERY SMALL or NO\"\n",
    "elif abs(effect_size) < 0.5:\n",
    "    magnitude = \"SMALL\"\n",
    "elif abs(effect_size) < 0.8:\n",
    "    magnitude = \"MEDIUM\"\n",
    "else:\n",
    "    magnitude = \"LARGE\"\n",
    "\n",
    "if effect_size > 0:\n",
    "    direction = \"Male words are more associated with CAREER\"\n",
    "else:\n",
    "    direction = \"Female words are more associated with CAREER\"\n",
    "\n",
    "print(f\"\\nEffect size: {effect_size:.3f}\")\n",
    "print(f\"Magnitude: {magnitude}\")\n",
    "print(f\"Direction: {direction}\")\n",
    "print(f\"\\nThis suggests the model has {magnitude} gender bias in\")\n",
    "print(f\"associating gender with career vs. family concepts.\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INTERPRETATION\n",
      "============================================================\n",
      "\n",
      "Effect size: 0.405\n",
      "Magnitude: SMALL\n",
      "Direction: Male words are more associated with CAREER\n",
      "\n",
      "This suggests the model has SMALL gender bias in\n",
      "associating gender with career vs. family concepts.\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
