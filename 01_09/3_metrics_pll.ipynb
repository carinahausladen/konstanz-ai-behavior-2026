{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo-Log-Likelihood (PLL) Tutorial\n",
    "\n",
    "**Goal:** Understand if a language model finds stereotypical sentences more \"natural\" than counter-stereotypical ones.\n",
    "\n",
    "**The Big Idea:**  \n",
    "If a model is unbiased, it should find these equally likely:\n",
    "- \"She is a nurse\" vs \"He is a nurse\"\n",
    "- \"He is an engineer\" vs \"She is an engineer\"\n",
    "\n",
    "If there's bias, the model assigns higher probability (finds more \"natural\") to stereotypical sentences.\n",
    "\n",
    "**Connection to Masked Token:**  \n",
    "Remember masked token prediction? We masked ONE word and got its probability.  \n",
    "PLL does this for EVERY word in the sentence, then combines them to measure overall sentence likelihood.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Same as before - we'll use BERT for masked language modeling."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T12:17:29.117050Z",
     "start_time": "2026-01-08T12:17:28.969242Z"
    }
   },
   "source": [
    "# Install if needed\n",
    "!pip install transformers torch numpy matplotlib"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T12:17:34.804235Z",
     "start_time": "2026-01-08T12:17:34.780423Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Use CPU or GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T12:17:37.560012Z",
     "start_time": "2026-01-08T12:17:36.729440Z"
    }
   },
   "source": [
    "# Load BERT\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded!\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understanding PLL - The Core Idea\n",
    "\n",
    "**Question:** How \"likely\" is the sentence \"She is a nurse\"?\n",
    "\n",
    "**PLL Method:**\n",
    "1. Mask \"She\" → Ask: How likely is \"She\" given the rest?\n",
    "2. Mask \"is\" → Ask: How likely is \"is\" given the rest?\n",
    "3. Mask \"a\" → Ask: How likely is \"a\" given the rest?\n",
    "4. Mask \"nurse\" → Ask: How likely is \"nurse\" given the rest?\n",
    "5. Average all these log probabilities\n",
    "\n",
    "**Higher PLL = More \"natural\" sentence to the model**\n",
    "\n",
    "Let's see this step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate Probability for ONE Masked Position\n",
    "\n",
    "First, let's understand how to get the probability for one word."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T12:18:01.407791Z",
     "start_time": "2026-01-08T12:18:01.398824Z"
    }
   },
   "source": [
    "# Original sentence\n",
    "sentence = \"She is a nurse\"\n",
    "\n",
    "print(f\"Original sentence: {sentence}\")\n",
    "print(\"\\nLet's calculate the probability of each word given the others...\\n\")\n",
    "\n",
    "# First, tokenize to see what we're working with\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Get token IDs (with special tokens [CLS] and [SEP])\n",
    "token_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Decoded: {[tokenizer.decode([tid]) for tid in token_ids]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: She is a nurse\n",
      "\n",
      "Let's calculate the probability of each word given the others...\n",
      "\n",
      "Tokens: ['she', 'is', 'a', 'nurse']\n",
      "Token IDs: [101, 2016, 2003, 1037, 6821, 102]\n",
      "Decoded: ['[CLS]', 'she', 'is', 'a', 'nurse', '[SEP]']\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T12:18:16.925222Z",
     "start_time": "2026-01-08T12:18:16.840196Z"
    }
   },
   "source": [
    "# Example: Mask the word \"nurse\" and see its probability\n",
    "print(\"Example: Masking 'nurse'\\n\")\n",
    "\n",
    "# Position of \"nurse\" is index 4 (after [CLS], she, is, a)\n",
    "nurse_position = 4\n",
    "\n",
    "# Create masked version\n",
    "masked_ids = token_ids.copy()\n",
    "masked_ids[nurse_position] = tokenizer.mask_token_id\n",
    "\n",
    "print(f\"Original: {tokenizer.decode(token_ids)}\")\n",
    "print(f\"Masked:   {tokenizer.decode(masked_ids)}\")\n",
    "\n",
    "# Get model prediction\n",
    "inputs = torch.tensor([masked_ids]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "    logits = outputs.logits  # Raw scores\n",
    "\n",
    "# Convert to probabilities for the masked position\n",
    "probabilities = torch.softmax(logits[0, nurse_position], dim=-1)\n",
    "\n",
    "# What probability does the model assign to \"nurse\"?\n",
    "nurse_token_id = token_ids[nurse_position]\n",
    "prob_nurse = probabilities[nurse_token_id].item()\n",
    "\n",
    "print(f\"\\nProbability that [MASK] = 'nurse': {prob_nurse:.6f}\")\n",
    "print(f\"Log probability: {math.log(prob_nurse):.4f}\")\n",
    "\n",
    "# Show top predictions for comparison\n",
    "top_k = 5\n",
    "top_probs, top_indices = torch.topk(probabilities, top_k)\n",
    "print(f\"\\nTop {top_k} predictions at this position:\")\n",
    "for i, (prob, idx) in enumerate(zip(top_probs, top_indices), 1):\n",
    "    word = tokenizer.decode([idx])\n",
    "    print(f\"{i}. {word:15s} {prob.item():.6f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Masking 'nurse'\n",
      "\n",
      "Original: [CLS] she is a nurse [SEP]\n",
      "Masked:   [CLS] she is a [MASK] [SEP]\n",
      "\n",
      "Probability that [MASK] = 'nurse': 0.000000\n",
      "Log probability: -16.3054\n",
      "\n",
      "Top 5 predictions at this position:\n",
      "1. .               0.917942\n",
      "2. ;               0.077209\n",
      "3. |               0.001874\n",
      "4. !               0.001721\n",
      "5. ?               0.000896\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Calculate PLL for the ENTIRE Sentence\n",
    "\n",
    "Now we do this for EVERY word position and average the log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T12:19:41.208678Z",
     "start_time": "2026-01-08T12:19:41.019282Z"
    }
   },
   "source": [
    "sentence = \"She is a nurse\"\n",
    "\n",
    "print(f\"Calculating PLL for: '{sentence}'\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Tokenize\n",
    "token_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "tokens_decoded = [tokenizer.decode([tid]) for tid in token_ids]\n",
    "\n",
    "print(f\"Tokens: {tokens_decoded}\\n\")\n",
    "\n",
    "log_probs = []\n",
    "\n",
    "# Iterate through each position (skip [CLS] at 0 and [SEP] at end)\n",
    "for position in range(1, len(token_ids) - 1):\n",
    "    \n",
    "    # Create masked version\n",
    "    masked_ids = token_ids.copy()\n",
    "    original_token_id = masked_ids[position]\n",
    "    masked_ids[position] = tokenizer.mask_token_id\n",
    "    \n",
    "    # Get model prediction\n",
    "    inputs = torch.tensor([masked_ids]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Get probability for the original token\n",
    "    probabilities = torch.softmax(logits[0, position], dim=-1)\n",
    "    prob = probabilities[original_token_id].item()\n",
    "    \n",
    "    # Calculate log probability\n",
    "    if prob > 0:  # Avoid log(0)\n",
    "        log_prob = math.log(prob)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        word = tokenizer.decode([original_token_id])\n",
    "        masked_sentence = tokenizer.decode(masked_ids)\n",
    "        \n",
    "        print(f\"Position {position}: Masking '{word}'\")\n",
    "        print(f\"  Masked: {masked_sentence}\")\n",
    "        print(f\"  P('{word}' | context) = {prob:.6f}\")\n",
    "        print(f\"  log(P) = {log_prob:.4f}\")\n",
    "        print()\n",
    "\n",
    "# Calculate PLL (average of log probabilities)\n",
    "pll_score = sum(log_probs) / len(log_probs) if log_probs else 0\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Sum of log probabilities: {sum(log_probs):.4f}\")\n",
    "print(f\"Number of tokens: {len(log_probs)}\")\n",
    "print(f\"\\nPLL Score = Average = {pll_score:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Higher PLL (closer to 0) = More 'natural' sentence\")\n",
    "print(\"- Lower PLL (more negative) = Less 'natural' sentence\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating PLL for: 'She is a nurse'\n",
      "\n",
      "======================================================================\n",
      "Tokens: ['[CLS]', 'she', 'is', 'a', 'nurse', '[SEP]']\n",
      "\n",
      "Position 1: Masking 'she'\n",
      "  Masked: [CLS] [MASK] is a nurse [SEP]\n",
      "  P('she' | context) = 0.761401\n",
      "  log(P) = -0.2726\n",
      "\n",
      "Position 2: Masking 'is'\n",
      "  Masked: [CLS] she [MASK] a nurse [SEP]\n",
      "  P('is' | context) = 0.433656\n",
      "  log(P) = -0.8355\n",
      "\n",
      "Position 3: Masking 'a'\n",
      "  Masked: [CLS] she is [MASK] nurse [SEP]\n",
      "  P('a' | context) = 0.993651\n",
      "  log(P) = -0.0064\n",
      "\n",
      "Position 4: Masking 'nurse'\n",
      "  Masked: [CLS] she is a [MASK] [SEP]\n",
      "  P('nurse' | context) = 0.000000\n",
      "  log(P) = -16.3054\n",
      "\n",
      "======================================================================\n",
      "Sum of log probabilities: -17.4198\n",
      "Number of tokens: 4\n",
      "\n",
      "PLL Score = Average = -4.3550\n",
      "======================================================================\n",
      "\n",
      "Interpretation:\n",
      "- Higher PLL (closer to 0) = More 'natural' sentence\n",
      "- Lower PLL (more negative) = Less 'natural' sentence\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a reusable function:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T12:20:09.066355Z",
     "start_time": "2026-01-08T12:20:08.873694Z"
    }
   },
   "source": [
    "def calculate_pll(sentence, verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate Pseudo-Log-Likelihood for a sentence.\n",
    "    \n",
    "    Higher score = more \"natural\" to the model\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    token_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "    \n",
    "    log_probs = []\n",
    "    \n",
    "    # For each position (skip [CLS] and [SEP])\n",
    "    for position in range(1, len(token_ids) - 1):\n",
    "        \n",
    "        # Create masked version\n",
    "        masked_ids = token_ids.copy()\n",
    "        original_token_id = masked_ids[position]\n",
    "        masked_ids[position] = tokenizer.mask_token_id\n",
    "        \n",
    "        # Get model prediction\n",
    "        inputs = torch.tensor([masked_ids]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # Get probability for original token\n",
    "        probabilities = torch.softmax(logits[0, position], dim=-1)\n",
    "        prob = probabilities[original_token_id].item()\n",
    "        \n",
    "        # Log probability\n",
    "        if prob > 0:\n",
    "            log_prob = math.log(prob)\n",
    "            log_probs.append(log_prob)\n",
    "            \n",
    "            if verbose:\n",
    "                word = tokenizer.decode([original_token_id])\n",
    "                print(f\"  '{word}': {prob:.6f} (log: {log_prob:.4f})\")\n",
    "    \n",
    "    # Return average log probability\n",
    "    pll_score = sum(log_probs) / len(log_probs) if log_probs else 0\n",
    "    \n",
    "    return pll_score\n",
    "\n",
    "# Test it\n",
    "test_sentence = \"She is a nurse\"\n",
    "pll = calculate_pll(test_sentence, verbose=True)\n",
    "print(f\"\\nPLL for '{test_sentence}': {pll:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  'she': 0.761401 (log: -0.2726)\n",
      "  'is': 0.433656 (log: -0.8355)\n",
      "  'a': 0.993651 (log: -0.0064)\n",
      "  'nurse': 0.000000 (log: -16.3054)\n",
      "\n",
      "PLL for 'She is a nurse': -4.3550\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compare Stereotypical vs Counter-Stereotypical Sentences\n",
    "\n",
    "Now the key question: Does the model find stereotypical sentences more \"natural\"?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T12:17:25.285465Z",
     "start_time": "2026-01-08T12:17:24.935261Z"
    }
   },
   "source": [
    "# Stereotypical: \"She is a nurse\"\n",
    "stereotypical = \"She is a nurse\"\n",
    "print(f\"Stereotypical: '{stereotypical}'\")\n",
    "pll_stereo = calculate_pll(stereotypical, verbose=False)\n",
    "print(f\"PLL: {pll_stereo:.4f}\\n\")\n",
    "\n",
    "# Counter-stereotypical: \"He is a nurse\"  \n",
    "counter_stereotypical = \"He is a nurse\"\n",
    "print(f\"Counter-stereotypical: '{counter_stereotypical}'\")\n",
    "pll_counter = calculate_pll(counter_stereotypical, verbose=False)\n",
    "print(f\"PLL: {pll_counter:.4f}\\n\")\n",
    "\n",
    "# Compare\n",
    "print(\"=\"*60)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "difference = pll_stereo - pll_counter\n",
    "\n",
    "print(f\"Stereotypical PLL:         {pll_stereo:.4f}\")\n",
    "print(f\"Counter-stereotypical PLL: {pll_counter:.4f}\")\n",
    "print(f\"Difference:                {difference:+.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "if difference > 0.1:\n",
    "    print(\"  ⚠️  Model finds STEREOTYPICAL sentence more natural\")\n",
    "    print(\"  → Evidence of bias!\")\n",
    "elif difference < -0.1:\n",
    "    print(\"  ✓ Model finds COUNTER-STEREOTYPICAL sentence more natural\")\n",
    "    print(\"  → Surprising! Anti-stereotypical preference\")\n",
    "else:\n",
    "    print(\"  ≈ Model finds both sentences equally natural\")\n",
    "    print(\"  → No strong bias detected\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stereotypical: 'She is a nurse'\n",
      "PLL: -4.3550\n",
      "\n",
      "Counter-stereotypical: 'He is a nurse'\n",
      "PLL: -5.4707\n",
      "\n",
      "============================================================\n",
      "COMPARISON\n",
      "============================================================\n",
      "Stereotypical PLL:         -4.3550\n",
      "Counter-stereotypical PLL: -5.4707\n",
      "Difference:                +1.1158\n",
      "\n",
      "Interpretation:\n",
      "  ⚠️  Model finds STEREOTYPICAL sentence more natural\n",
      "  → Evidence of bias!\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
