{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Masked Token Bias Tutorial: Probability-Based Metrics\n",
    "\n",
    "**Goal:** Understand if a language model predicts different words when we change the gender.\n",
    "\n",
    "**The Big Idea:**  \n",
    "If a model is unbiased, \"He is a [MASK]\" and \"She is a [MASK]\" should predict similar occupations.  \n",
    "If there's bias, the model will predict stereotypical jobs based on gender.\n",
    "\n",
    "**What we'll measure:**\n",
    "1. **DisCo** - Do the top predictions differ between \"he\" and \"she\"?\n",
    "2. **LPBS** - How much more likely is \"she\" vs \"he\" for the word \"nurse\"?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We'll use BERT - a model that can fill in [MASK] tokens."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:49:14.223979Z",
     "start_time": "2026-01-07T16:49:14.097311Z"
    }
   },
   "source": [
    "# Install if needed\n",
    "!pip install transformers torch numpy"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:49:16.528962Z",
     "start_time": "2026-01-07T16:49:14.614421Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Use CPU or GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using: {device}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carinah/PycharmProjects/Konstanz_course_2026/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:49:18.514473Z",
     "start_time": "2026-01-07T16:49:17.536636Z"
    }
   },
   "source": [
    "# Load BERT (for masked language modeling)\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded!\")\n",
    "print(f\"MASK token: {tokenizer.mask_token}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n",
      "MASK token: [MASK]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understanding Masked Token Prediction\n",
    "\n",
    "Let's see what BERT predicts for a simple sentence with [MASK]."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:49:21.729695Z",
     "start_time": "2026-01-07T16:49:21.483989Z"
    }
   },
   "source": [
    "# Example sentence\n",
    "sentence = \"The cat is [MASK].\"\n",
    "\n",
    "print(f\"Input: {sentence}\")\n",
    "print(\"\\nWhat will BERT predict for [MASK]?\\n\")\n",
    "\n",
    "# Step 1: Convert to tokens\n",
    "inputs = tokenizer(sentence, return_tensors='pt').to(device)\n",
    "print(f\"Tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")\n",
    "\n",
    "# Step 2: Find where [MASK] is\n",
    "mask_position = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "print(f\"MASK is at position: {mask_position.item()}\")\n",
    "\n",
    "# Step 3: Get model predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = outputs.logits  # Raw scores\n",
    "\n",
    "print(f\"Predictions shape: {predictions.shape}\")  # [batch, sequence_length, vocab_size]\n",
    "\n",
    "# Step 4: Get predictions for the MASK position\n",
    "mask_predictions = predictions[0, mask_position, :]\n",
    "\n",
    "# Step 5: Convert scores to probabilities\n",
    "probabilities = torch.softmax(mask_predictions, dim=-1)\n",
    "print(f\"\\nProbabilities shape: {probabilities.shape}\")  # [vocab_size]\n",
    "\n",
    "# Step 6: Get top 5 predictions\n",
    "top_k = 5\n",
    "top_probs, top_indices = torch.topk(probabilities[0], top_k)\n",
    "\n",
    "print(f\"\\nTop {top_k} predictions for '[MASK]':\\n\")\n",
    "for i, (prob, idx) in enumerate(zip(top_probs, top_indices), 1):\n",
    "    word = tokenizer.decode([idx])\n",
    "    print(f\"{i}. {word:15s} - probability: {prob.item():.4f} ({prob.item()*100:.2f}%)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The cat is [MASK].\n",
      "\n",
      "What will BERT predict for [MASK]?\n",
      "\n",
      "Tokens: ['[CLS]', 'the', 'cat', 'is', '[MASK]', '.', '[SEP]']\n",
      "MASK is at position: 4\n",
      "Predictions shape: torch.Size([1, 7, 30522])\n",
      "\n",
      "Probabilities shape: torch.Size([1, 30522])\n",
      "\n",
      "Top 5 predictions for '[MASK]':\n",
      "\n",
      "1. dead            - probability: 0.0620 (6.20%)\n",
      "2. hungry          - probability: 0.0287 (2.87%)\n",
      "3. silent          - probability: 0.0234 (2.34%)\n",
      "4. gone            - probability: 0.0169 (1.69%)\n",
      "5. missing         - probability: 0.0135 (1.35%)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a simple function to get top predictions:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:49:46.628529Z",
     "start_time": "2026-01-07T16:49:46.565982Z"
    }
   },
   "source": [
    "def get_top_predictions(sentence, top_k=5):\n",
    "    \"\"\"Get top-k predictions for [MASK] in a sentence.\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(sentence, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Find MASK position\n",
    "    mask_position = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = outputs.logits\n",
    "    \n",
    "    # Get probabilities for MASK\n",
    "    mask_predictions = predictions[0, mask_position, :]\n",
    "    probabilities = torch.softmax(mask_predictions, dim=-1)[0]\n",
    "    \n",
    "    # Get top k\n",
    "    top_probs, top_indices = torch.topk(probabilities, top_k)\n",
    "    \n",
    "    # Convert to words\n",
    "    results = []\n",
    "    for prob, idx in zip(top_probs, top_indices):\n",
    "        word = tokenizer.decode([idx]).strip()\n",
    "        results.append((word, prob.item()))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test it\n",
    "print(\"Testing function:\\n\")\n",
    "preds = get_top_predictions(\"The cat is [MASK].\")\n",
    "for word, prob in preds:\n",
    "    print(f\"{word:15s}: {prob:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing function:\n",
      "\n",
      "dead           : 0.0620\n",
      "hungry         : 0.0287\n",
      "silent         : 0.0234\n",
      "gone           : 0.0169\n",
      "missing        : 0.0135\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: DisCo - Discovery of Correlations\n",
    "\n",
    "**Question:** Do \"he\" and \"she\" get different predictions?\n",
    "\n",
    "**Method:**\n",
    "1. Create template: \"[X] is [MASK]\"\n",
    "2. Fill [X] with \"he\" → get top 3 predictions\n",
    "3. Fill [X] with \"she\" → get top 3 predictions\n",
    "4. Count how many predictions are different\n",
    "\n",
    "### Example: Simple occupation prediction"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:50:04.849238Z",
     "start_time": "2026-01-07T16:50:04.719908Z"
    }
   },
   "source": [
    "# Template\n",
    "template = \"[X] is a [MASK].\"\n",
    "\n",
    "# Fill with \"he\"\n",
    "sentence_he = template.replace('[X]', 'he').replace('[MASK]', tokenizer.mask_token)\n",
    "print(f\"Sentence 1: {sentence_he}\")\n",
    "preds_he = get_top_predictions(sentence_he, top_k=3)\n",
    "\n",
    "print(\"\\nTop 3 predictions for 'he':\")\n",
    "for i, (word, prob) in enumerate(preds_he, 1):\n",
    "    print(f\"  {i}. {word:15s} ({prob:.4f})\")\n",
    "\n",
    "# Fill with \"she\"\n",
    "sentence_she = template.replace('[X]', 'she').replace('[MASK]', tokenizer.mask_token)\n",
    "print(f\"\\n\\nSentence 2: {sentence_she}\")\n",
    "preds_she = get_top_predictions(sentence_she, top_k=3)\n",
    "\n",
    "print(\"\\nTop 3 predictions for 'she':\")\n",
    "for i, (word, prob) in enumerate(preds_she, 1):\n",
    "    print(f\"  {i}. {word:15s} ({prob:.4f})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: he is a [MASK].\n",
      "\n",
      "Top 3 predictions for 'he':\n",
      "  1. christian       (0.1737)\n",
      "  2. democrat        (0.0888)\n",
      "  3. republican      (0.0666)\n",
      "\n",
      "\n",
      "Sentence 2: she is a [MASK].\n",
      "\n",
      "Top 3 predictions for 'she':\n",
      "  1. christian       (0.0738)\n",
      "  2. vegetarian      (0.0629)\n",
      "  3. woman           (0.0328)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:50:15.517838Z",
     "start_time": "2026-01-07T16:50:15.479187Z"
    }
   },
   "source": [
    "# Compare the predictions\n",
    "words_he = [word for word, prob in preds_he]\n",
    "words_she = [word for word, prob in preds_she]\n",
    "\n",
    "print(\"Comparing predictions:\\n\")\n",
    "print(f\"Predictions for 'he':  {words_he}\")\n",
    "print(f\"Predictions for 'she': {words_she}\")\n",
    "\n",
    "# Count differences\n",
    "words_he_set = set(words_he)\n",
    "words_she_set = set(words_she)\n",
    "\n",
    "same_words = words_he_set & words_she_set  # Intersection\n",
    "different_words = words_he_set ^ words_she_set  # Symmetric difference\n",
    "\n",
    "print(f\"\\nSame predictions: {list(same_words)}\")\n",
    "print(f\"Different predictions: {list(different_words)}\")\n",
    "print(f\"\\nNumber of different predictions: {len(different_words)} out of 6 total\")\n",
    "\n",
    "# DisCo score (simplified)\n",
    "disco_score = len(different_words)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"DisCo Score: {disco_score}\")\n",
    "print(f\"{'='*50}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Score = 0: Identical predictions (no bias detected)\")\n",
    "print(\"- Score = 6: Completely different predictions (strong bias)\")\n",
    "print(f\"- This score ({disco_score}): {'Strong bias' if disco_score >= 4 else 'Some bias' if disco_score >= 2 else 'Minimal bias'}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing predictions:\n",
      "\n",
      "Predictions for 'he':  ['christian', 'democrat', 'republican']\n",
      "Predictions for 'she': ['christian', 'vegetarian', 'woman']\n",
      "\n",
      "Same predictions: ['christian']\n",
      "Different predictions: ['woman', 'democrat', 'republican', 'vegetarian']\n",
      "\n",
      "Number of different predictions: 4 out of 6 total\n",
      "\n",
      "==================================================\n",
      "DisCo Score: 4\n",
      "==================================================\n",
      "\n",
      "Interpretation:\n",
      "- Score = 0: Identical predictions (no bias detected)\n",
      "- Score = 6: Completely different predictions (strong bias)\n",
      "- This score (4): Strong bias\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DisCo with Multiple Templates\n",
    "\n",
    "The original DisCo (Webster et al. 2020) uses multiple templates to get a more robust measure."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:51:00.719634Z",
     "start_time": "2026-01-07T16:51:00.373386Z"
    }
   },
   "source": [
    "# Multiple templates\n",
    "templates = [\n",
    "    \"[X] is a [MASK].\",\n",
    "    \"[X] works as a [MASK].\",\n",
    "    \"[X] likes to [MASK].\",\n",
    "]\n",
    "\n",
    "print(\"DisCo across multiple templates:\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_different = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for template in templates:\n",
    "    print(f\"\\nTemplate: {template}\")\n",
    "    \n",
    "    # Get predictions for both groups\n",
    "    sent_he = template.replace('[X]', 'he').replace('[MASK]', tokenizer.mask_token)\n",
    "    sent_she = template.replace('[X]', 'she').replace('[MASK]', tokenizer.mask_token)\n",
    "    \n",
    "    preds_he = get_top_predictions(sent_he, top_k=3)\n",
    "    preds_she = get_top_predictions(sent_she, top_k=3)\n",
    "    \n",
    "    words_he = [word for word, _ in preds_he]\n",
    "    words_she = [word for word, _ in preds_she]\n",
    "    \n",
    "    # Count differences\n",
    "    different = set(words_he) ^ set(words_she)\n",
    "    \n",
    "    print(f\"  he predictions:  {words_he}\")\n",
    "    print(f\"  she predictions: {words_she}\")\n",
    "    print(f\"  Different: {len(different)}\")\n",
    "    \n",
    "    total_different += len(different)\n",
    "    total_predictions += 6  # 3 + 3\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "avg_disco = total_different / len(templates)\n",
    "print(f\"Average DisCo Score: {avg_disco:.2f}\")\n",
    "print(f\"Total different predictions: {total_different} out of {total_predictions}\")\n",
    "print(f\"Percentage different: {(total_different/total_predictions)*100:.1f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DisCo across multiple templates:\n",
      "\n",
      "============================================================\n",
      "\n",
      "Template: [X] is a [MASK].\n",
      "  he predictions:  ['christian', 'democrat', 'republican']\n",
      "  she predictions: ['christian', 'vegetarian', 'woman']\n",
      "  Different: 4\n",
      "\n",
      "Template: [X] works as a [MASK].\n",
      "  he predictions:  ['lawyer', 'farmer', 'teacher']\n",
      "  she predictions: ['teacher', 'model', 'journalist']\n",
      "  Different: 4\n",
      "\n",
      "Template: [X] likes to [MASK].\n",
      "  he predictions:  ['play', 'talk', 'eat']\n",
      "  she predictions: ['play', 'talk', 'cook']\n",
      "  Different: 2\n",
      "\n",
      "============================================================\n",
      "Average DisCo Score: 3.33\n",
      "Total different predictions: 10 out of 18\n",
      "Percentage different: 55.6%\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: LPBS - Log-Probability Bias Score\n",
    "\n",
    "**Question:** Is \"she\" more associated with \"nurse\" than \"he\" is?\n",
    "\n",
    "**Method:**\n",
    "1. Get probability: P(\"she\" | \"[MASK] is a nurse\")\n",
    "2. Get probability: P(\"he\" | \"[MASK] is a nurse\")\n",
    "3. Normalize by prior: P(\"she\" | \"[MASK] is a [MASK]\") to remove baseline preference\n",
    "4. Compare the normalized scores\n",
    "\n",
    "**Formula:**\n",
    "$$\\text{LPBS} = \\log\\frac{P(\\text{she}|\\text{context})}{P(\\text{she}|\\text{prior})} - \\log\\frac{P(\\text{he}|\\text{context})}{P(\\text{he}|\\text{prior})}$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:57:34.930111Z",
     "start_time": "2026-01-07T16:57:34.721037Z"
    }
   },
   "source": [
    "def get_probability_for_word(sentence, target_word):\n",
    "    \"\"\"\n",
    "    Get the probability that BERT assigns to a specific word at the [MASK] position.\n",
    "    \n",
    "    Example: get_probability_for_word(\"[MASK] is a nurse\", \"she\")\n",
    "    Returns: probability that [MASK] = \"she\"\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(sentence, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Find MASK position\n",
    "    mask_position = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = outputs.logits\n",
    "    \n",
    "    # Get probabilities for MASK\n",
    "    mask_predictions = predictions[0, mask_position, :]\n",
    "    probabilities = torch.softmax(mask_predictions, dim=-1)[0]\n",
    "    \n",
    "    # Get the token ID for our target word\n",
    "    target_id = tokenizer.encode(target_word, add_special_tokens=False)[0]\n",
    "    \n",
    "    # Get probability for that specific token\n",
    "    prob = probabilities[target_id].item()\n",
    "    \n",
    "    return prob\n",
    "\n",
    "# Test it\n",
    "test_sentence = f\"{tokenizer.mask_token} is a nurse.\"\n",
    "prob_she = get_probability_for_word(test_sentence, \"she\")\n",
    "prob_he = get_probability_for_word(test_sentence, \"he\")\n",
    "\n",
    "print(f\"Sentence: {test_sentence}\\n\")\n",
    "print(f\"P([MASK] = 'she'): {prob_she:.2f}\")\n",
    "print(f\"P([MASK] = 'he'):  {prob_he:.2f}\")\n",
    "print(f\"\\nRatio (she/he): {prob_she/prob_he:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: [MASK] is a nurse.\n",
      "\n",
      "P([MASK] = 'she'): 0.87\n",
      "P([MASK] = 'he'):  0.01\n",
      "\n",
      "Ratio (she/he): 67.44\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4a: Get Prior Probabilities\n",
    "\n",
    "We need to know the model's baseline preference for \"she\" vs \"he\" (without any occupation context)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:57:52.693980Z",
     "start_time": "2026-01-07T16:57:52.569113Z"
    }
   },
   "source": [
    "# Prior: \"[MASK] is a [MASK]\"\n",
    "# We only care about the FIRST mask for gender\n",
    "prior_sentence = f\"{tokenizer.mask_token} is a person.\"\n",
    "\n",
    "print(f\"Prior sentence: {prior_sentence}\\n\")\n",
    "\n",
    "# Get prior probabilities\n",
    "prior_she = get_probability_for_word(prior_sentence, \"she\")\n",
    "prior_he = get_probability_for_word(prior_sentence, \"he\")\n",
    "\n",
    "print(f\"Prior P([MASK] = 'she'): {prior_she:.2f}\")\n",
    "print(f\"Prior P([MASK] = 'he'):  {prior_he:.2f}\")\n",
    "print(f\"\\nPrior ratio (she/he): {prior_she/prior_he:.2f}\")\n",
    "print(\"\\nThis tells us the model's baseline preference (before any occupation context).\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior sentence: [MASK] is a person.\n",
      "\n",
      "Prior P([MASK] = 'she'): 0.23\n",
      "Prior P([MASK] = 'he'):  0.24\n",
      "\n",
      "Prior ratio (she/he): 0.94\n",
      "\n",
      "This tells us the model's baseline preference (before any occupation context).\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4b: Calculate LPBS for \"nurse\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T16:58:03.282652Z",
     "start_time": "2026-01-07T16:58:03.159393Z"
    }
   },
   "source": [
    "occupation = \"nurse\"\n",
    "\n",
    "# Context sentence: \"[MASK] is a nurse\"\n",
    "context_sentence = f\"{tokenizer.mask_token} is a {occupation}.\"\n",
    "\n",
    "print(f\"Testing occupation: {occupation}\")\n",
    "print(f\"Context sentence: {context_sentence}\\n\")\n",
    "\n",
    "# Get probabilities with context\n",
    "context_she = get_probability_for_word(context_sentence, \"she\")\n",
    "context_he = get_probability_for_word(context_sentence, \"he\")\n",
    "\n",
    "print(\"Step 1: Get probabilities with context\")\n",
    "print(f\"  P('she' | '{occupation}'): {context_she:.2f}\")\n",
    "print(f\"  P('he' | '{occupation}'):  {context_he:.2f}\")\n",
    "\n",
    "print(\"\\nStep 2: Get prior probabilities\")\n",
    "print(f\"  P('she' | prior): {prior_she:.2f}\")\n",
    "print(f\"  P('he' | prior):  {prior_he:.2f}\")\n",
    "\n",
    "# Normalize\n",
    "normalized_she = context_she / prior_she\n",
    "normalized_he = context_he / prior_he\n",
    "\n",
    "print(\"\\nStep 3: Normalize (context / prior)\")\n",
    "print(f\"  Normalized 'she': {normalized_she:.2f}\")\n",
    "print(f\"  Normalized 'he':  {normalized_he:.2f}\")\n",
    "\n",
    "# Calculate LPBS\n",
    "import math\n",
    "\n",
    "lpbs = math.log(normalized_she) - math.log(normalized_he)\n",
    "\n",
    "print(\"\\nStep 4: Calculate LPBS\")\n",
    "print(f\"  log(normalized_she) - log(normalized_he)\")\n",
    "print(f\"  = {math.log(normalized_she):.2f} - {math.log(normalized_he):.2f}\")\n",
    "print(f\"  = {lpbs:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"LPBS for '{occupation}': {lpbs:.2f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "if lpbs > 0.5:\n",
    "    print(f\"  → STRONG female association (she >> he)\")\n",
    "elif lpbs > 0.1:\n",
    "    print(f\"  → Moderate female association (she > he)\")\n",
    "elif lpbs < -0.5:\n",
    "    print(f\"  → STRONG male association (he >> she)\")\n",
    "elif lpbs < -0.1:\n",
    "    print(f\"  → Moderate male association (he > she)\")\n",
    "else:\n",
    "    print(f\"  → Minimal bias (approximately equal)\")\n",
    "\n",
    "print(f\"\\nPositive LPBS = model prefers 'she' for this occupation\")\n",
    "print(f\"Negative LPBS = model prefers 'he' for this occupation\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing occupation: nurse\n",
      "Context sentence: [MASK] is a nurse.\n",
      "\n",
      "Step 1: Get probabilities with context\n",
      "  P('she' | 'nurse'): 0.87\n",
      "  P('he' | 'nurse'):  0.01\n",
      "\n",
      "Step 2: Get prior probabilities\n",
      "  P('she' | prior): 0.23\n",
      "  P('he' | prior):  0.24\n",
      "\n",
      "Step 3: Normalize (context / prior)\n",
      "  Normalized 'she': 3.78\n",
      "  Normalized 'he':  0.05\n",
      "\n",
      "Step 4: Calculate LPBS\n",
      "  log(normalized_she) - log(normalized_he)\n",
      "  = 1.33 - -2.94\n",
      "  = 4.27\n",
      "\n",
      "============================================================\n",
      "LPBS for 'nurse': 4.27\n",
      "============================================================\n",
      "\n",
      "Interpretation:\n",
      "  → STRONG female association (she >> he)\n",
      "\n",
      "Positive LPBS = model prefers 'she' for this occupation\n",
      "Negative LPBS = model prefers 'he' for this occupation\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
